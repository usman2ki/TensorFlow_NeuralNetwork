{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95563c8d-eeda-460c-a6f7-c526cc388b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc747de-729c-4288-af6c-c5dd31eb9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8678b9-d2ef-41ff-a985-52ea524342f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples. Features standardized into X_norm.\n",
      "Data Split: Train=350, Val=75, Test=75\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. SETUP: DATA LOADING AND PREPROCESSING \n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data Loading and Standardization ---\n",
    "try:\n",
    "    # 1. Load data, assuming 'house_prices_dataset.csv' is available\n",
    "    df = pd.read_csv('house_prices_dataset.csv')\n",
    "    \n",
    "    # Logic to identify features and targets\n",
    "    feature_columns = ['area_scaled', 'rooms_scaled', 'location_score_scaled']\n",
    "    target_column = 'price'\n",
    "    \n",
    "    if not all(col in df.columns for col in feature_columns + [target_column]):\n",
    "        print(f\"Error: CSV must contain columns: {feature_columns + [target_column]}\")\n",
    "        exit()\n",
    "        \n",
    "    X = df[feature_columns].values \n",
    "    y = df[target_column].values.reshape(-1, 1)\n",
    "    \n",
    "    # 2. Apply StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_norm = scaler.fit_transform(X) \n",
    "    print(f\"Loaded {X.shape[0]} samples. Features standardized into X_norm.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'house_prices_dataset.csv' not found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Data Splitting ---\n",
    "# We use a standard 70/15/15 split.\n",
    "TEST_SIZE = 0.15 # 15% for final testing\n",
    "VAL_RATIO = 0.17647 # (0.15 / 0.85) = 17.647% of the remaining 85% for validation\n",
    "RANDOM_SEED = 42 # Consistent seed for reproducibility\n",
    "\n",
    "# First split: Separate Test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_norm, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: Separate Train and Validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=VAL_RATIO, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Data Split: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "\n",
    "# --- Define Hyperparameters  ---\n",
    "INPUT_SIZE = X_train.shape[1] \n",
    "\n",
    "ACTIVATIONS = ['relu', 'leaky_relu', 'elu', 'selu', 'gelu', 'swish'] # >= 6 required\n",
    "DEPTHS = [1, 2, 3] # >= 3 hidden layers required\n",
    "WIDTHS = [8, 16, 32] # >= 3 widths required (6 x 3 x 3 = 54 runs minimum)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 150 # Max epochs before early stopping\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 15 # Early stopping patience based on val_loss\n",
    "\n",
    "results_list = [] # List to store results for the final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610a90b3-91b0-4a93-b6e1-fdf3e8ff4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. MODEL CREATION FUNCTION (Keras FNN Model)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_model(depth, width, activation_name):\n",
    "    \"\"\"Dynamically creates the FNN model based on parameters.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Determine the activation for the dense layers\n",
    "    if activation_name == 'leaky_relu':\n",
    "        # [cite_start]LeakyReLU must be implemented as a layer with alpha=0.01 [cite: 1, 17]\n",
    "        activation_layer = keras.layers.LeakyReLU(alpha=0.01)\n",
    "        use_layer = True\n",
    "    else:\n",
    "        # Standard activations are available as strings\n",
    "        activation_layer = activation_name \n",
    "        use_layer = False\n",
    "\n",
    "    # Build Hidden Layers\n",
    "    for _ in range(depth):\n",
    "        if use_layer:\n",
    "            # Add Dense layer then the LeakyReLU layer\n",
    "            model.add(keras.layers.Dense(width))\n",
    "            model.add(activation_layer)\n",
    "        else:\n",
    "            # Use activation string for other standard functions\n",
    "            model.add(keras.layers.Dense(width, activation=activation_layer))\n",
    "            \n",
    "    # Output Layer (1 neuron, linear activation for regression)\n",
    "    model.add(keras.layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compilation\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    # [cite_start]Loss: MSE is the primary metric [cite: 1, 25]\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b273f551-0b7f-4ce0-83bf-c1a5c79ae76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=relu, Depth=1, Width=8\n",
      "Running: Act=relu, Depth=1, Width=16\n",
      "Running: Act=relu, Depth=1, Width=32\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000164679FD760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000164679FD760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running: Act=relu, Depth=2, Width=8\n",
      "Running: Act=relu, Depth=2, Width=16\n",
      "Running: Act=relu, Depth=2, Width=32\n",
      "Running: Act=relu, Depth=3, Width=8\n",
      "Running: Act=relu, Depth=3, Width=16\n",
      "Running: Act=relu, Depth=3, Width=32\n",
      "Running: Act=leaky_relu, Depth=1, Width=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=1, Width=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=1, Width=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=2, Width=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=2, Width=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=2, Width=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=3, Width=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      " warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=3, Width=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=leaky_relu, Depth=3, Width=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Act=elu, Depth=1, Width=8\n",
      "Running: Act=elu, Depth=1, Width=16\n",
      "Running: Act=elu, Depth=1, Width=32\n",
      "Running: Act=elu, Depth=2, Width=8\n",
      "Running: Act=elu, Depth=2, Width=16\n",
      "Running: Act=elu, Depth=2, Width=32\n",
      "Running: Act=elu, Depth=3, Width=8\n",
      "Running: Act=elu, Depth=3, Width=16\n",
      "Running: Act=elu, Depth=3, Width=32\n",
      "Running: Act=selu, Depth=1, Width=8\n",
      "Running: Act=selu, Depth=1, Width=16\n",
      "Running: Act=selu, Depth=1, Width=32\n",
      "Running: Act=selu, Depth=2, Width=8\n",
      "Running: Act=selu, Depth=2, Width=16\n",
      "Running: Act=selu, Depth=2, Width=32\n",
      "Running: Act=selu, Depth=3, Width=8\n",
      "Running: Act=selu, Depth=3, Width=16\n",
      "Running: Act=selu, Depth=3, Width=32\n",
      "Running: Act=gelu, Depth=1, Width=8\n",
      "Running: Act=gelu, Depth=1, Width=16\n",
      "Running: Act=gelu, Depth=1, Width=32\n",
      "Running: Act=gelu, Depth=2, Width=8\n",
      "Running: Act=gelu, Depth=2, Width=16\n",
      "Running: Act=gelu, Depth=2, Width=32\n",
      "Running: Act=gelu, Depth=3, Width=8\n",
      "Running: Act=gelu, Depth=3, Width=16\n",
      "Running: Act=gelu, Depth=3, Width=32\n",
      "Running: Act=swish, Depth=1, Width=8\n",
      "Running: Act=swish, Depth=1, Width=16\n",
      "Running: Act=swish, Depth=1, Width=32\n",
      "Running: Act=swish, Depth=2, Width=8\n",
      "Running: Act=swish, Depth=2, Width=16\n",
      "Running: Act=swish, Depth=2, Width=32\n",
      "Running: Act=swish, Depth=3, Width=8\n",
      "Running: Act=swish, Depth=3, Width=16\n",
      "Running: Act=swish, Depth=3, Width=32\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. PHASE 2: THE BENCHMARK EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "# Dictionary to store loss histories for required plots\n",
    "history_storage = {}\n",
    "best_val_r2 = -float('inf')\n",
    "best_overall_models = [] # To store the top 3 models for loss curves\n",
    "\n",
    "for activation in ACTIVATIONS:\n",
    "    for depth in DEPTHS:\n",
    "        for width in WIDTHS:\n",
    "            \n",
    "            print(f\"Running: Act={activation}, Depth={depth}, Width={width}\")\n",
    "            \n",
    "            # --- Initialize Logging ---\n",
    "            config = {\n",
    "                'activation': activation,\n",
    "                'depth': depth,\n",
    "                'width': width\n",
    "            }\n",
    "            \n",
    "            # --- Model Creation and Parameter Count ---\n",
    "            model = create_model(depth, width, activation)\n",
    "            # --- FIX: Explicitly build the model before counting parameters ---\n",
    "            # X_train is a 2D array (samples, features). We need the shape of ONE sample.\n",
    "            input_shape = (None, X_train.shape[1]) \n",
    "            # The (None, ...) handles the batch size, and X_train.shape[1] is the number of features (3)\n",
    "\n",
    "            model.build(input_shape)\n",
    "            n_params = model.count_params()\n",
    "            config['n_params'] = n_params \n",
    "\n",
    "            # [cite_start]--- Training with Runtime Tracking and Early Stopping [cite: 1, 24] ---\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Callback to save best model based on validation loss, restoring weights\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=PATIENCE, \n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            config['runtime_sec'] = end_time - start_time\n",
    "            \n",
    "            # [cite_start]--- Extract Metrics [cite: 1, 25] ---\n",
    "            \n",
    "            # 1. Best Validation Loss and Epoch\n",
    "            best_val_mse = min(history.history['val_loss'])\n",
    "            best_epoch = np.argmin(history.history['val_loss']) + 1 \n",
    "            config['val_mse'] = best_val_mse\n",
    "            config['best_epoch'] = best_epoch\n",
    "            \n",
    "            # [cite_start]2. Evaluate on Test Set (MSE, MAE, R2) [cite: 1, 25]\n",
    "            test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "            test_r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            config['test_mse'] = test_loss\n",
    "            config['test_mae'] = test_mae\n",
    "            config['test_r2'] = test_r2\n",
    "            \n",
    "            # --- Store History for Loss Curves (Deliverable 3b) ---\n",
    "            history_storage[f'{activation}_{depth}_{width}'] = history.history\n",
    "\n",
    "            # --- Track Overall Top 3 Models ---\n",
    "            # Update the list of best models based on test_r2\n",
    "            model_summary = {\n",
    "                'name': f'{activation}_D{depth}_W{width}',\n",
    "                'test_r2': test_r2,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            # Keep the top 3 models based on R2 (higher is better)\n",
    "            best_overall_models.append(model_summary)\n",
    "            best_overall_models.sort(key=lambda x: x['test_r2'], reverse=True)\n",
    "            best_overall_models = best_overall_models[:3]\n",
    "            \n",
    "            # --- Append to Results ---\n",
    "            results_list.append(config)\n",
    "            \n",
    "# [cite_start]--- Save Results to CSV (Deliverable) [cite: 2, 35] ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# File will be written in the next code execution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadfa219-31bd-4582-8872-28a9b350a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results saved to 'feedforward_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Assuming results_df is generated from the previous code block\n",
    "results_df.to_csv('feedforward_benchmark_results.csv', index=False)\n",
    "print(\"Benchmark results saved to 'feedforward_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a9e2c7-060e-4093-b9e8-83dd299525db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps generated (one per activation function).\n"
     ]
    }
   ],
   "source": [
    "# Load the generated results CSV (if running separately)\n",
    "# results_df = pd.read_csv('feedforward_benchmark_results.csv')\n",
    "\n",
    "# --- Generate Heatmaps ---\n",
    "for activation in results_df['activation'].unique():\n",
    "    # Filter data for the current activation\n",
    "    df_act = results_df[results_df['activation'] == activation]\n",
    "    \n",
    "    # Pivot the data: Depth (index) vs. Width (columns) with val_mse (values)\n",
    "    heatmap_data = df_act.pivot(index='depth', columns='width', values='val_mse')\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        annot=True, \n",
    "        fmt=\".3f\", \n",
    "        cmap=\"coolwarm_r\", # '_r' reverses the colormap, so better (lower) MSE is darker\n",
    "        cbar_kws={'label': 'Best Validation MSE'}\n",
    "    )\n",
    "    #[cite_start]\n",
    "    plt.title(f'Validation MSE Benchmark: {activation.upper()}')\n",
    "    plt.xlabel('Width (Neurons per Layer)')\n",
    "    plt.ylabel('Depth (Hidden Layers)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'heatmap_{activation}_val_mse.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"Heatmaps generated (one per activation function).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da34bde8-4288-4e45-9b82-0a91569f66b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared Bar Chart generated.\n"
     ]
    }
   ],
   "source": [
    "# --- Find Best Model per Activation for R2 comparison ---\n",
    "# Group by activation and find the row with the maximum test_r2 in each group\n",
    "best_r2_per_activation = results_df.loc[results_df.groupby('activation')['test_r2'].idxmax()]\n",
    "\n",
    "# --- Generate R2 Bar Chart ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='activation', \n",
    "    y='test_r2', \n",
    "    data=best_r2_per_activation.sort_values(by='test_r2', ascending=False)\n",
    ")\n",
    "plt.title('Best Test R-Squared ($R^2$) by Activation Function')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('Test $R^2$ (Higher is Better)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('bar_chart_best_r2_by_activation.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"R-Squared Bar Chart generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b79570-f173-49ba-a949-d5a0afd88e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Track Overall Top 3 Models ---\n",
    "model_summary = {\n",
    "    'name': f'{activation}_D{depth}_W{width}',\n",
    "    'test_r2': test_r2,\n",
    "    'history': history.history  # <--- THIS IS THE CRITICAL DATA\n",
    "}\n",
    "\n",
    "# Keep the top 3 models based on R2 (higher is better)\n",
    "best_overall_models.append(model_summary)\n",
    "best_overall_models.sort(key=lambda x: x['test_r2'], reverse=True)\n",
    "best_overall_models = best_overall_models[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e78a50f-119f-4f9c-827c-09ab6f8b0993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Loss Curves for Top 3 Models...\n",
      "Generated: loss_curve_top_1_leaky_relu_D3_W32.png\n",
      "Generated: loss_curve_top_2_relu_D3_W32.png\n",
      "Generated: loss_curve_top_3_gelu_D3_W32.png\n",
      "\n",
      "All Top-3 Loss Curves generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the 'best_overall_models' list still exists in your notebook's memory\n",
    "# from when you ran the main execution loop.\n",
    "\n",
    "# --- Function to plot loss curves ---\n",
    "def plot_loss_history(history_dict, model_name, plot_filename):\n",
    "    \"\"\"Plots training and validation loss curves for a single model.\"\"\"\n",
    "    epochs = range(1, len(history_dict['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, history_dict['loss'], 'b-', label='Training Loss (MSE)')\n",
    "    plt.plot(epochs, history_dict['val_loss'], 'r-', label='Validation Loss (MSE)')\n",
    "    \n",
    "    # Highlight the best epoch (where validation loss was lowest)\n",
    "    best_val_loss = min(history_dict['val_loss'])\n",
    "    best_epoch = np.argmin(history_dict['val_loss']) + 1\n",
    "    \n",
    "    plt.axvline(best_epoch, color='k', linestyle='--', alpha=0.5, label=f'Early Stop Epoch ({best_epoch})')\n",
    "\n",
    "    plt.title(f'Loss Convergence: {model_name} (Best Val MSE: {best_val_loss:.2f})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Mean Squared Error)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Generating Loss Curves for Top 3 Models...\")\n",
    "    \n",
    "# --- Iterate and Plot Top 3 Models ---\n",
    "for i, model_summary in enumerate(best_overall_models):\n",
    "    model_name = model_summary['name']\n",
    "    history_data = model_summary['history']\n",
    "    filename = f'loss_curve_top_{i+1}_{model_name}.png'\n",
    "    \n",
    "    plot_loss_history(history_data, model_name, filename)\n",
    "    print(f\"Generated: {filename}\")\n",
    "\n",
    "print(\"\\nAll Top-3 Loss Curves generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcb93d-c35d-4946-b609-4cdb9a1fdfc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
